{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef2e95-a06f-44ac-a8cc-3cbc30bf0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcdaa4-5fbf-4e83-a15a-94a71e4b2a53",
   "metadata": {},
   "source": [
    "Part 1. Data Pre-processing and Exploration\n",
    "-\n",
    "A) Reading files and cleaning data. \n",
    "-\n",
    "Notes about reading files and changes\n",
    "1. Data for each activity was stored as separate list of dataframes to simplify code, make it readable, and for easy split to test/train later in 1B. e.g., all bending csv files were read into list \"bending1_dfs\" and so on. \n",
    "2. Column 1 was renamed to \"time\" since it default read as \"# Columns: time\"\n",
    "3. We add a column \"activity\" that stores which activity the data belongs to, so that when we later combine the dataframes into 1, we retain this info.\n",
    "4. Dataset 4 in Bending2 had mixed delimiters: comma space in header and space delimiter in data with some trailing spaces.\n",
    "   Thus, this was treated separately\n",
    "5. 1st 4 rows were skipped as it did not contain relevant data\n",
    "6. Some datasets had extra columns due to trailing spaces. Thus we used usecols=range(7) to ensure we only read the 1st 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf519f91-c34f-45e8-a31d-377c5f3c42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Read bending1 files: 7 csv files\n",
    "bending1_dfs = [pd.read_csv(f\"../data/arem/bending1/dataset{i}.csv\", sep=',', skiprows=4) for i in range(1, 8)]\n",
    "bending1_dfs = [df.rename(columns={df.columns[0]: \"time\"}) for df in bending1_dfs]\n",
    "for df in bending1_dfs:\n",
    "    df[\"activity\"] = \"bending1\"\n",
    "\n",
    "# Read bending2 (6 files) \n",
    "# dataset4.csv: Headers have comma delmiter, data has space delimiter. thus we read headers separately and data separately. \n",
    "bending2_dfs = []\n",
    "for i in range(1, 7):\n",
    "    file_path = f\"../data/arem/bending2/dataset{i}.csv\"\n",
    "    if i == 4:\n",
    "        # Read only the column names first.\n",
    "        headers = pd.read_csv(file_path, sep=',', skiprows=4, nrows=1).columns.tolist()\n",
    "        # Read the actual data using space separator and correct headers\n",
    "        df = pd.read_csv(file_path, sep=r\"\\s+\", skiprows=5, names=headers, engine=\"python\")\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, sep=',', skiprows=4, usecols=range(7))\n",
    "    df.rename(columns={df.columns[0]: \"time\"}, inplace=True)\n",
    "    df[\"activity\"] = \"bending2\"\n",
    "    bending2_dfs.append(df)\n",
    "\n",
    "# Read cycling (15 files) using usecols to ensure column consistency\n",
    "cycling_dfs = [pd.read_csv(f\"../data/arem/cycling/dataset{i}.csv\", sep=',', skiprows=4, usecols=range(7)) for i in range(1, 16)]\n",
    "cycling_dfs = [df.rename(columns={df.columns[0]: \"time\"}) for df in cycling_dfs]\n",
    "for df in cycling_dfs:\n",
    "    df[\"activity\"] = \"cycling\"\n",
    "\n",
    "\n",
    "# Read lying, sitting, standing, walking (15 files)\n",
    "lying_dfs = [pd.read_csv(f\"../data/arem/lying/dataset{i}.csv\", sep=',', skiprows=4, usecols=range(7)) for i in range(1, 16)]\n",
    "lying_dfs = [df.rename(columns={df.columns[0]: \"time\"}) for df in lying_dfs]\n",
    "for df in lying_dfs:\n",
    "    df[\"activity\"] = \"lying\"\n",
    "\n",
    "sitting_dfs = [pd.read_csv(f\"../data/arem/sitting/dataset{i}.csv\", sep=',', skiprows=4, usecols=range(7)) for i in range(1, 16)]\n",
    "sitting_dfs = [df.rename(columns={df.columns[0]: \"time\"}) for df in sitting_dfs]\n",
    "for df in sitting_dfs:\n",
    "    df[\"activity\"] = \"sitting\"\n",
    "\n",
    "standing_dfs = [pd.read_csv(f\"../data/arem/standing/dataset{i}.csv\", sep=',', skiprows=4, usecols=range(7)) for i in range(1, 16)]\n",
    "standing_dfs = [df.rename(columns={df.columns[0]: \"time\"}) for df in standing_dfs]\n",
    "for df in standing_dfs:\n",
    "    df[\"activity\"] = \"standing\"\n",
    "\n",
    "walking_dfs = [pd.read_csv(f\"../data/arem/walking/dataset{i}.csv\", sep=',', skiprows=4, usecols=range(7)) for i in range(1, 16)]\n",
    "walking_dfs = [df.rename(columns={df.columns[0]: \"time\"}) for df in walking_dfs]\n",
    "for df in walking_dfs:\n",
    "    df[\"activity\"] = \"walking\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addf639-71ca-4e58-8661-7fab15abe855",
   "metadata": {},
   "source": [
    "1B) Split train and test data. \n",
    "-\n",
    "Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1,\n",
    "2, and 3 in other folders as test data and other datasets as train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376c9ce-3d93-4486-b768-d1b0bdb47dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = (\n",
    "    bending1_dfs[:2] +\n",
    "    bending2_dfs[:2] +\n",
    "    cycling_dfs[:3] +\n",
    "    lying_dfs[:3] +\n",
    "    sitting_dfs[:3] +\n",
    "    standing_dfs[:3] +\n",
    "    walking_dfs[:3]\n",
    ")\n",
    "\n",
    "train_dfs = (\n",
    "    bending1_dfs[2:] +\n",
    "    bending2_dfs[2:] +\n",
    "    cycling_dfs[3:] +\n",
    "    lying_dfs[3:] +\n",
    "    sitting_dfs[3:] +\n",
    "    standing_dfs[3:] +\n",
    "    walking_dfs[3:]\n",
    ")\n",
    "# Combining list of dataframes into single dataset using concat. \n",
    "#ignore_index = True so that we dont end up with the original file based index files which can be duplicate \n",
    "\n",
    "train_data = pd.concat(train_dfs, ignore_index = True)\n",
    "test_data = pd.concat(test_dfs, ignore_index = True) \n",
    "\n",
    "print(train_data.info())\n",
    "print(test_data.info())\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4c13e-5989-47e9-b45a-7a5ac8dc1ad7",
   "metadata": {},
   "source": [
    "1C) Extract Time Domain Features\n",
    "-\n",
    "I. Common time domain features used in time series: mean, median, max, min, variance, mode, standard deviation\n",
    "- Other features: Kurtosis, Skew, Quantile/Percentile-function, Autocorrelation, Correlation.\n",
    "Source: https://medium.com/@dreamferus/time-series-feature-extraction-using-pandas-44af6fb5fce9\n",
    "\n",
    "II. Feature Extraction: Compute min, max, median, std, 1st quartile, 3rd quartile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5512a3-9b0e-43eb-a7eb-fd5db5d9b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.width\", 300)\n",
    "pd.set_option(\"display.max_columns\", 15)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # Limit decimal places\n",
    "\n",
    "result = []\n",
    "combined_dfs = train_dfs + test_dfs\n",
    "index = 1\n",
    "for df in combined_dfs: \n",
    "    row = [index]\n",
    "    for j in df.columns[1:7]: #excludes time and activity columns\n",
    "        row.append(np.min(df[j]))\n",
    "        row.append(np.max(df[j]))\n",
    "        row.append(np.mean(df[j]))\n",
    "        row.append(np.median(df[j]))\n",
    "        row.append(np.std(df[j]))\n",
    "        row.append(np.percentile(df[j], 25))\n",
    "        row.append(np.percentile(df[j], 75))\n",
    "    result.append(row)\n",
    "    index = index + 1\n",
    "\n",
    "result_columns = [\"instance\"] + [f\"{name}{i+1}\" for i in range(6) for name in [\"min\",\"max\",\"mean\",\"median\",\"std\",\"1st_quartile\",\"3rd_quartile\"]]\n",
    "result_data = pd.DataFrame(result, columns=result_columns)\n",
    "\n",
    "print(result_data)    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c94493-f1be-4ba6-ba99-c709992cab3d",
   "metadata": {},
   "source": [
    "1D) Computing standard deviation bootstrap confidence interval for each of the 42 extracted features. \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9c355-4d4e-4920-9d97-be82b841680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option(\"display.width\", 300)\n",
    "pd.set_option(\"display.max_columns\", 15)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # Limit decimal places\n",
    "\n",
    "# Extract feature columns from result_data (excluding 'instance' column)\n",
    "feature_columns = []\n",
    "for col in result_data.columns:\n",
    "    if col!= \"instance\":\n",
    "        feature_columns.append(col)\n",
    "        \n",
    "\n",
    "feature_names = []\n",
    "std_devs = []\n",
    "lower_bounds = []\n",
    "upper_bounds = []\n",
    "\n",
    "# Compute standard deviation and bootstrap confidence intervals for each extracted time-domain feature\n",
    "for col in feature_columns:\n",
    "    data = result_data[col].dropna().values\n",
    "    std_dev = np.std(data)\n",
    "    ci = stats.bootstrap((data,), np.std, confidence_level=0.90, n_resamples=1000, method='percentile')\n",
    "    #ddof: sample: to make unbiased estimator, we take n-1 degrees of freedom; python std lookup \n",
    "    \n",
    "    # Append values to lists\n",
    "    feature_names.append(col)\n",
    "    std_devs.append(std_dev)\n",
    "    lower_bounds.append(ci.confidence_interval.low)\n",
    "    upper_bounds.append(ci.confidence_interval.high)\n",
    "\n",
    "\n",
    "# Convert results to DataFrame \n",
    "summary_data = pd.DataFrame({\n",
    "    \"Standard Deviation\": std_devs,\n",
    "    \"Lower Bound\": lower_bounds,\n",
    "    \"Upper Bound\": upper_bounds\n",
    "}, index=feature_columns)\n",
    "\n",
    "print(\"\\nFeature Summary with Standard Deviations and Bootstrap Confidence Intervals:\")\n",
    "print(summary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88c66a-5bfa-4b91-9807-9ebd22013c1b",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "***************************************************************************************************************************************\n",
    "\n",
    "\n",
    "E) Pairplots\n",
    "---------------\n",
    "\n",
    "First we will use the training data frames to extract the time domain features.\n",
    "We select features mean, median, std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da190d-818b-43c6-8f8b-4c086248251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.width\", 300)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # Limit decimal places\n",
    "\n",
    "result = []\n",
    "index = 1\n",
    "for df in train_dfs: \n",
    "    row = [index]\n",
    "    row.append(np.mean(df[\"avg_rss12\"]))\n",
    "    row.append(np.median(df[\"avg_rss12\"]))\n",
    "    row.append(np.std(df[\"avg_rss12\"]))\n",
    "\n",
    "    row.append(np.mean(df[\"var_rss12\"]))\n",
    "    row.append(np.median(df[\"var_rss12\"]))\n",
    "    row.append(np.std(df[\"var_rss12\"]))\n",
    "    \n",
    "    row.append(np.mean(df[\"var_rss23\"]))\n",
    "    row.append(np.median(df[\"var_rss23\"]))\n",
    "    row.append(np.std(df[\"var_rss23\"]))\n",
    "    row.append(df[\"activity\"].iloc[0])\n",
    "    \n",
    "    result.append(row)\n",
    "    index = index + 1\n",
    "\n",
    "result_columns = [\"instance\", \"mean1\", \"median1\", \"std1\", \"mean2\", \"median2\",\"std2\", \"mean6\", \"median6\", \"std6\", \"activity\"]\n",
    "train_result = pd.DataFrame(result, columns=result_columns)\n",
    "train_result[\"bending_vs_other\"] = \"other\"\n",
    "train_result.loc[train_result[\"activity\"].isin([\"bending1\",\"bending2\"]), \"bending_vs_other\"] = \"bending\"\n",
    "print(train_result) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccd681-92ac-4c00-8985-5a2c1c7ffb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating pairplot of 9 x 9 variables\n",
    "\n",
    "\n",
    "selected_features = [\"mean1\", \"median1\", \"std1\", \"mean2\", \"median2\",\"std2\", \"mean6\", \"median6\", \"std6\"]\n",
    "sns.pairplot(train_result, vars = selected_features, hue = \"bending_vs_other\", height = 1.5) \n",
    "plt.suptitle(\"Pairplot: 9 selected features\", y = 1.02, fontsize = 16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67b597-c950-42d6-ac77-7c2fdafc4546",
   "metadata": {},
   "source": [
    "Pairplots after splitting data to test / train\n",
    " -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0df82-4d1f-46e4-99bb-f136b431e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.width\", 300)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # Limit decimal places\n",
    "\n",
    "result_split = []\n",
    "index = 1\n",
    "\n",
    "for df in train_dfs:\n",
    "    row = [index]\n",
    "    mid = len(df) // 2\n",
    "\n",
    "    #Extracting features from 1st half of each training instance\n",
    "    row.append(np.mean(df[\"avg_rss12\"][:mid]))\n",
    "    row.append(np.median(df[\"avg_rss12\"][:mid]))\n",
    "    row.append(np.std(df[\"avg_rss12\"][:mid]))\n",
    "\n",
    "    row.append(np.mean(df[\"var_rss12\"][:mid]))\n",
    "    row.append(np.median(df[\"var_rss12\"][:mid]))\n",
    "    row.append(np.std(df[\"var_rss12\"][:mid]))\n",
    "\n",
    "    row.append(np.mean(df[\"var_rss23\"][:mid]))\n",
    "    row.append(np.median(df[\"var_rss23\"][:mid]))\n",
    "    row.append(np.std(df[\"var_rss23\"][:mid]))\n",
    "\n",
    "    #Extracting features from 2nd half of each training instance\n",
    "    row.append(np.mean(df[\"avg_rss12\"][mid:]))\n",
    "    row.append(np.median(df[\"avg_rss12\"][mid:]))\n",
    "    row.append(np.std(df[\"avg_rss12\"][mid:]))\n",
    "\n",
    "    row.append(np.mean(df[\"var_rss12\"][mid:]))\n",
    "    row.append(np.median(df[\"var_rss12\"][mid:]))\n",
    "    row.append(np.std(df[\"var_rss12\"][mid:]))\n",
    "\n",
    "    row.append(np.mean(df[\"var_rss23\"][mid:]))\n",
    "    row.append(np.median(df[\"var_rss23\"][mid:]))\n",
    "    row.append(np.std(df[\"var_rss23\"][mid:]))\n",
    "\n",
    "    row.append(df[\"activity\"].iloc[0])\n",
    "\n",
    "    result_split.append(row)\n",
    "    index+=1 \n",
    "\n",
    "result_columns = [\n",
    "    \"instance\", \"mean1_half1\", \"median1_half1\", \"std1_half1\", \"mean2_half1\", \"median2_half1\", \"std2_half1\", \n",
    "    \"mean6_half1\", \"median6_half1\", \"std6_half1\", \"mean1_half2\", \"median1_half2\", \"std1_half2\", \n",
    "    \"mean2_half2\", \"median2_half2\", \"std2_half2\", \"mean6_half2\", \"median6_half2\", \"std6_half2\", \"activity\"\n",
    "]\n",
    "\n",
    "train_result_split = pd.DataFrame(result_split, columns = result_columns)\n",
    "\n",
    "train_result_split[\"bending_vs_other\"] = \"other\"\n",
    "train_result_split.loc[train_result[\"activity\"].isin([\"bending1\", \"bending2\"]), \"bending_vs_other\"] = \"bending\"\n",
    "\n",
    "print(train_result_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d306a-71aa-4d54-8bec-9c2daef8ef0b",
   "metadata": {},
   "source": [
    "Splitting the 18x18 pairplot to 2 9x9 pairplots for better readability (it will be difficult to fit 18x18 in one image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556f41a-9544-4fb3-9ea7-9c2b6f91b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features1 = [\n",
    "    \"mean1_half1\", \"median1_half1\", \"std1_half1\", \"mean2_half1\", \"median2_half1\", \"std2_half1\",\n",
    "    \"mean6_half1\", \"median6_half1\", \"std6_half1\"\n",
    "]\n",
    "\n",
    "selected_features2 = [\n",
    "    \"mean1_half2\", \"median1_half2\", \"std1_half2\", \"mean2_half2\", \"median2_half2\", \"std2_half2\",\n",
    "    \"mean6_half2\", \"median6_half2\", \"std6_half2\"\n",
    "]\n",
    "\n",
    "#\n",
    "g1 = sns.pairplot(train_result_split, vars = selected_features1, hue = \"bending_vs_other\", height = 1.5)\n",
    "plt.suptitle(\"Pairplot: First 9 Features\", y= 1.02, fontsize = 14) \n",
    "plt.show()\n",
    "\n",
    "g2 = sns.pairplot(train_result_split, vars = selected_features2, hue = \"bending_vs_other\", height = 1.5)\n",
    "plt.suptitle(\"Pairplot: Second 9 Features\", y= 1.02, fontsize = 14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de1580-0346-4886-90cc-580a7f78b7a9",
   "metadata": {},
   "source": [
    "Observations\n",
    "-\n",
    "On initial observation, there werenâ€™t many overall differences between the plots generated in 2a and 2b, there were some differences were more pronounced for bending features datapoints. For example:\n",
    "1. bending pairplot plots for mean1 and std1 look different for the split vs. non-split dfs \n",
    "2. for median6 vs. std6, plots are different for 2a) and 2b) for bending points\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Part 2. MODEL FIT\n",
    "---\n",
    "\n",
    "A) Model 1: Logistic Regression and Recursive Feature Selection (RFECV) using Cross Validation \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Cross Validation and Feature selection right Way and Wrong way : Source (ESL Textbook 7.10.2)\n",
    "\n",
    "Wrong way: Performing Feature Selection before Cross-validation. i.e., Feature selection applied to entire dataset before splitting into train-test folds.\n",
    "\n",
    "This is wrong because data leakage occurs, i.e. test set indirectly influences selected features and model is trained and tested on already optimized feature set leading to overly optimistic results\n",
    "\n",
    "Right way: Perform feature selection within each training fold ensuring test data remains unseen. This avoids information leakage by selecting features inside each fold. \n",
    "\n",
    "\n",
    "Overall breakdown of steps for 2)a)iii\n",
    "\n",
    "1. Extract Features from Time Series Data\n",
    "\n",
    "   a) We split dataset into l equal segments (l = 1 to 20).\n",
    "\n",
    "   b) Compute 7 domain features: Min, Max, Mean, Median, Std, 25th Percentile, 75th Percentile.\n",
    "\n",
    "   c) Store extracted features in a matrix X with labels in y. Labels are assigned as 1 for bending and 0 for other activities. Also assign feature names as avg_var12_mean_seg0 etc. where avg_var12 represents time series, mean represents time domain feature and seg0 represents segment number from 0 to 19. \n",
    "\n",
    "\n",
    "2. Stratified k-fold Cross Validation\n",
    "\n",
    "    a) Use 5 fold cross validation. Functions used: StratifiedKFold.n_splits = 5: use 5 folds. Prevents class imbalance. \n",
    "\n",
    "\n",
    "3. Logistic Regression and Recursive Feature Selection (RFECV)\n",
    "\n",
    "    a) Function LogisticRegression() used with max_iter = 100, C=1e6 to remove effect of L2 regularization. Stored in model. \n",
    "\n",
    "    b) Within each fold, RFECV selects most important features. The average across folds is stored in rfecv.cv_results_['mean_test_score'].        Note that each value in rfecv.cv_results_['mean_test_score'] represents 1 subset size and the mean score corresponding to that              subset size across 5 folds\n",
    "\n",
    "    c) We choose the max of these test scores to select optimal feature size. rfecv.n_features_ returns the number of features corresponding to the max test score.\n",
    "\n",
    "\n",
    "4. Finding best l with p \n",
    "\n",
    "    a) Store test accuracy, number of selected features, and test error (1-accuracy) for each l.\n",
    "\n",
    "    b) Out of all the l used above, track best segmentation (best_l) and feature count p.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afa03a-0b3c-42b3-8a3c-9807357fa75a",
   "metadata": {},
   "source": [
    "Function to extract features for modularity (called by multiple models)  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff4cab-5ba8-4e4c-8969-c8b93412e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(train_dfs, l):\n",
    "    \"\"\"\n",
    "    Extract time-series features for a value l.  \n",
    "    Returns:\n",
    "    feature_matrix: A NumPy array where each row is a feature vector for one dataset. \n",
    "    labels: NumPy array where each entry is class label (1 for bending, 0 for others)\n",
    "    Feature names: list of feature names (column headers) \n",
    "    \"\"\"\n",
    "    feature_matrix = [] #stores feature data for all dataframes. initially empty. \n",
    "    labels = []\n",
    "    feature_names = [] #stores feature names \n",
    "        \n",
    "    for df in train_dfs: #Loop through every training dataframe\n",
    "        labels.append(1 if df[\"activity\"].iloc[0] in [\"bending1\", \"bending2\"] else 0) #assign labels as 1 for bending and 0 for all others\n",
    "\n",
    "        #Generate l+1 evenly spaced indices for each dataframe which will be boundaries for each segment. This will depend on l value.\n",
    "        #Thus, the indices would be the split points for each dataframe. \n",
    "        indices = np.linspace(0, len(df), l+1, dtype = int)\n",
    "\n",
    "        features = [] #to store computed features for current dataset. Reset to empty for each df. \n",
    "\n",
    "        for i in range(l): #loops over l segments and extracts subset of rows (segments)\n",
    "            segment = df.iloc[indices[i]: indices[i+1]]\n",
    "\n",
    "            for col in df.columns: \n",
    "                if col not in [\"activity\", \"time\"]: #Exclude activity and time columns\n",
    "                    # Define feature names for every segment\n",
    "                    feature_names.append(f\"{col}_min_seg{i}\")\n",
    "                    feature_names.append(f\"{col}_max_seg{i}\")\n",
    "                    feature_names.append(f\"{col}_mean_seg{i}\")\n",
    "                    feature_names.append(f\"{col}_median_seg{i}\")\n",
    "                    feature_names.append(f\"{col}_std_seg{i}\")\n",
    "                    feature_names.append(f\"{col}_p25_seg{i}\")\n",
    "                    feature_names.append(f\"{col}_p75_seg{i}\")\n",
    "                    features.extend([\n",
    "                        np.min(segment[col]), np.max(segment[col]), np.mean(segment[col]),\n",
    "                        np.median(segment[col]), np.std(segment[col]),\n",
    "                        np.percentile(segment[col], 25), np.percentile(segment[col], 75)\n",
    "                    ])\n",
    "        #each datafram produces one row (features) which contains all computed features across all segments. \n",
    "        #This row is appended to feature_matrix as a single instance. Thus each dataframe represent a single row or instance.\n",
    "        #Final shape of feature_matrix is (number of dataframes: 69 in training, number of features: depends on l) \n",
    "        feature_matrix.append(features)   \n",
    "       \n",
    "    #returning np.arrays for efficiency  and because LogisticRegression and RFECV expect NumPy arrays. \n",
    "    return np.array(feature_matrix), np.array(labels), feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e1315-d12a-447a-a56a-c0b3a984c677",
   "metadata": {},
   "source": [
    "Function to train Logistic Regression using Cross Validation and Feature Selection.\n",
    "--\n",
    "\n",
    "Note: we used RFECV for feature selection and not P-values for feature extraction\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1d921-0205-418e-a3a5-3344d34cd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.float_format\", \"{:.5f}\".format)  # Limit decimal places\n",
    "def train_logistic_reg(X, y, feature_names, cv_splits = 5):\n",
    "    \"\"\"\n",
    "    Trains Logistic Regression with RFECV for feature selection. \n",
    "    Returns trained model, feature selector, and test accuracy. \n",
    "    \"\"\"\n",
    "    #Using StratifiedKFold for imbalanced dataset. Shuffle = false to ensure time order preserved. \n",
    "    cv = StratifiedKFold(n_splits = cv_splits, shuffle = True, random_state = 41) \n",
    "\n",
    "    selected_features = [] #Stores number of features selected in each fold\n",
    "    fold_scores = [] #Stores test accuracy for each fold \n",
    "     \n",
    "    model = LogisticRegression(C=1e6, max_iter = 100, solver = \"lbfgs\") \n",
    "    #max_iter = 1000 -> ensures enough iterations for convergence\n",
    "    #solver = lbfgs (optimizer for small to medium datasets) \n",
    "    \n",
    "\n",
    "    rfecv = RFECV(estimator= model, cv=cv, scoring=\"accuracy\")\n",
    "    #RFECV automatically selects best features using cross-validation within each fold\n",
    "    #scoring = 'accuracy': feature importance based on classification accuracy\n",
    "\n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    \n",
    "    # Get optimal number of features\n",
    "    optimal_num_features = rfecv.n_features_\n",
    "\n",
    "    # Get optimal feature names using selected feature indices\n",
    "    optimal_indices = np.where(rfecv.support_)[0]\n",
    "    optimal_features = [feature_names[i] for i in optimal_indices]\n",
    "    \n",
    "    # Get the best accuracy score from CV\n",
    "    test_score = rfecv.cv_results_['mean_test_score'].max()\n",
    "\n",
    "\n",
    "\n",
    "    return model, optimal_features, test_score, optimal_num_features\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6db694-5752-4756-ab86-a888b8271d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.float_format\", \"{:.5f}\".format)  # Limit decimal places\n",
    "l_values = list(range(1,21))\n",
    "\n",
    "best_score = -np.inf\n",
    "best_l = None\n",
    "best_num_features = None\n",
    "\n",
    "#Prepare dataset with different 'l' values. Each row will have value of l, test accuracy, number of selected features, and test error. \n",
    "results = []\n",
    "\n",
    "#Train model for each 'l'\n",
    "for l in l_values:\n",
    "    X, y, feature_names = extract_features(train_dfs, l) #Splits dataset into l segments and computes 7 time domain features per segment. \n",
    "    model, selected_features, avg_score, avg_num_features = train_logistic_reg(X, y, feature_names)\n",
    "    results.append((l, avg_score, avg_num_features, 1 - avg_score, selected_features))\n",
    "    print(f\"Code completed execution for l={l}\")\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_l = l\n",
    "        best_num_features = avg_num_features\n",
    "        best_features = selected_features\n",
    "        \n",
    "# Print best results\n",
    "print(f\"Best l: {best_l}, Best number of selected features (p): {best_num_features}\")\n",
    "print(f\"Best features: {best_features}\")\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}, Best test error: {1 - best_score:.4f}\")\n",
    "\n",
    "# Print summary table\n",
    "summary_df = pd.DataFrame(results, columns=[\"l\", \"Train Accuracy\", \"Num Features\", \"Train Error\", \"Selected Features\"])\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794238b4-064d-4b14-af46-2dee0fe6963a",
   "metadata": {},
   "source": [
    "\n",
    "Best train accuracy = 1.0. Best l and p combination: l: 13, Best number of selected features (p): 15\n",
    "--\n",
    "\n",
    "\n",
    "\n",
    "Confusion Matrix, ROC, AUC, p-Values\n",
    "-----------------------------------\n",
    "\n",
    "Training Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015ae1e-a29d-44c1-96b2-976487efaadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train model on best l\n",
    "X_optimal_train, y_optimal_train, all_feature_names = extract_features(train_dfs, best_l)\n",
    "selected_indices = [all_feature_names.index(f) for f in best_features]  # Get indices of selected features\n",
    "X_optimal_train = X_optimal_train[:, selected_indices]  # Keep only selected features\n",
    "\n",
    "# Logistic Regression Coefficients and P-values for Best l\n",
    "X_optimal_train_const = sm.add_constant(X_optimal_train)  # Add intercept\n",
    "logit_model = sm.Logit(y_optimal_train, X_optimal_train_const)\n",
    "summary = logit_model.fit(method='bfgs').summary()\n",
    "print(summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4649afa-2c2e-4be4-9e70-2336abe89598",
   "metadata": {},
   "source": [
    "NOTE TO TAs:  DUE TO SYSTEM THAT I AM RUNNING THIS ON, P-VALUES ARE NOT COMPUTER CORRECTLY. I HAVE ALSO DISCUSSED THIS IN TA OFFICE HOURS WITH DAKSH. \n",
    "\n",
    "Function to evaluate model performance on train set. Called again later in 2B. \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fe4cb-f670-468e-add4-a2a6c6b9233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_eval(final_model, X_optimal_train, y_optimal_train): \n",
    "    \"\"\"\n",
    "    Calculates model accuracy, confusion matrix, ROC curve and AUC on training data.  \n",
    "    Defining as a function here so it can be re-used later. \n",
    "\n",
    "    \"\"\"\n",
    "    #Calculating Train accuracy\n",
    "    final_model.fit(X_optimal_train, y_optimal_train)\n",
    "    y_optimal_pred = final_model.predict(X_optimal_train)\n",
    "    train_score = final_model.score(X_optimal_train, y_optimal_train)\n",
    "    print(\"RESULTS FOR TRAIN DATA\")\n",
    "    print (\"Train accuracy: \", train_score)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    c_matrix = confusion_matrix(y_optimal_train, y_optimal_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(c_matrix)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    y_prob = final_model.predict_proba(X_optimal_train)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_optimal_train, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "train_model_eval(model, X_optimal_train, y_optimal_train) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8285f16-6380-4a73-a1c2-af86a6bedbe1",
   "metadata": {},
   "source": [
    "Testing the classifier on test set. \n",
    "----\n",
    "\n",
    "\n",
    "Function to evaluate model performance on test set for modularity. Called by multiple models. \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa495d-8f04-4bb9-b170-f519561bbb84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model_eval(final_model, X_optimal_test, y_optimal_test): \n",
    "    \"\"\"\n",
    "    Calculates model accuracy, confusion matrix, ROC curve and AUC on test data.  \n",
    "    Defining as a function here so it can be re-used later. \n",
    "\n",
    "    \"\"\"\n",
    "    #Calculating Test accuracy\n",
    "    print(\"RESULTS FOR TEST DATA: \")\n",
    "    y_optimal_pred = final_model.predict(X_optimal_test)\n",
    "    test_score = final_model.score(X_optimal_test, y_optimal_test)\n",
    "    print (\"Test accuracy: \", test_score)\n",
    "    \n",
    "    \n",
    "    # Confusion Matrix\n",
    "    c_matrix = confusion_matrix(y_optimal_test, y_optimal_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(c_matrix)\n",
    "\n",
    "    # ROC Curve and AUC\n",
    "    y_prob = final_model.predict_proba(X_optimal_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_optimal_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "#First we extract test set features using the best l\n",
    "X_optimal_test, y_optimal_test, all_feature_names = extract_features(test_dfs, best_l)\n",
    "selected_indices = [all_feature_names.index(f) for f in best_features]  # Get indices of selected features\n",
    "X_optimal_test = X_optimal_test[:, selected_indices]  # Keep only selected features\n",
    "\n",
    "test_model_eval(model, X_optimal_test, y_optimal_test) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781d76b-0459-4371-bfc4-8fc81b7258d5",
   "metadata": {},
   "source": [
    " Findings: Accuracy score for train and test set is same: 1.0\n",
    "----\n",
    "\n",
    "Complete Separation: The results show that there is complete separation or perfect prediction. \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197a798-6d18-48c1-95ea-1c396376ec63",
   "metadata": {},
   "source": [
    "Data has imbalance in it. Class 0 has 60 whereas class 1 is only 9 based on the training confusion matrix. \n",
    "This can bias the model towards predicting non-bending cases. \n",
    "--\n",
    "\n",
    "Upsampling\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2249387-bffc-4aa0-ab95-b3572cc958fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def case_control_sampling(X, y):\n",
    "    \"\"\"\n",
    "    Implements case-control sampling by selecting an equal number of cases (bending) and controls (other activities).\n",
    "    If fewer than `target_samples` exist in a class, it defaults to the minimum available class size.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(X)\n",
    "    df['label'] = y\n",
    "\n",
    "    majority_class = df[df['label'] == 0]\n",
    "    minority_class = df[df['label'] == 1]\n",
    "\n",
    "    # Sample equal number of cases and controls\n",
    "    majority_sampled = resample(df[df['label'] == 0], replace=True, n_samples=len(majority_class), random_state=41)\n",
    "    minority_sampled = resample(df[df['label'] == 1], replace=True, n_samples=len(majority_class), random_state=41)\n",
    "    \n",
    "    balanced_df = pd.concat([majority_sampled, minority_sampled])\n",
    "    \n",
    "    X_balanced = balanced_df.drop(columns=['label']).to_numpy()\n",
    "    y_balanced = balanced_df['label'].to_numpy()\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Re-run feature extraction and model training after balancing the dataset\n",
    "l_values = list(range(1, 21))\n",
    "\n",
    "best_score = -np.inf\n",
    "best_l = None\n",
    "best_num_features = None\n",
    "\n",
    "# Prepare dataset with different 'l' values\n",
    "results = []\n",
    "\n",
    "for l in l_values:\n",
    "    X, y, all_feature_names = extract_features(train_dfs, l)\n",
    "    X_balanced, y_balanced = case_control_sampling(X, y)\n",
    "    balanced_model, selected_features, avg_score, avg_num_features = train_logistic_reg(X_balanced, y_balanced, all_feature_names)\n",
    "    results.append((l, avg_score, avg_num_features, 1 - avg_score, selected_features))\n",
    "    print(\"Execution completed for l: \", l)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_l = l\n",
    "        best_num_features = avg_num_features\n",
    "        best_features = selected_features\n",
    "\n",
    "# Print best results\n",
    "print(f\"Best l: {best_l}, Best number of selected features (p): {best_num_features}\")\n",
    "print(f\"Best features: {best_features}\")\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}, Best train error: {1 - best_score:.4f}\")\n",
    "\n",
    "# Print summary table\n",
    "summary_df = pd.DataFrame(results, columns=[\"l\", \"Train Accuracy\", \"Num Features\", \"Train Error\", \"Selected Features\"])\n",
    "print(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b438c-4da3-4335-adda-fd3cbe9e0ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repeat model score, confusion matrix, ROC, AUC for training  set for the new model with case control sampling. \n",
    "# Train model on best l\n",
    "X_raw_train, y_raw_train, all_feature_names = extract_features(train_dfs, best_l)\n",
    "X_optimal_train, y_optimal_train = case_control_sampling(X_raw_train, y_raw_train)\n",
    "selected_indices = [all_feature_names.index(f) for f in best_features]  # Get indices of selected features\n",
    "X_optimal_train = X_optimal_train[:, selected_indices]  # Keep only selected features\n",
    "train_model_eval(balanced_model, X_optimal_train, y_optimal_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c5070-e7c9-47b1-82b3-3b265c539fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat model score confusion matrix, ROC, AUC for test set for the new model with case control sampling. \n",
    "X_optimal_test, y_optimal_test, all_feature_names = extract_features(test_dfs, best_l)\n",
    "selected_indices = [all_feature_names.index(f) for f in best_features]  # Get indices of selected features\n",
    "X_optimal_test = X_optimal_test[:, selected_indices]  # Keep only selected features\n",
    "test_model_eval(balanced_model, X_optimal_test, y_optimal_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64289b2b-96c4-4475-aad2-cad61eed3fdb",
   "metadata": {},
   "source": [
    "After adjusting for sample imbalance, train and test accuracy scores are still 1.0. Best l = 1, p = 4\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17030059-ba41-4653-b340-582f3f223fbd",
   "metadata": {},
   "source": [
    "B) Model 2: BINARY CLASSIFICATION USING L1 REGULARIZATION \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daef66f-bf0a-42df-abd2-bb294c475303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l1_logistic_reg(X, y, feature_names, cv_splits=5):\n",
    "    \"\"\"\n",
    "    Trains L1-penalized Logistic Regression using L1 regularization for feature selection.\n",
    "    Returns trained model, selected features, and test accuracy. \n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=41)\n",
    "    model = LogisticRegressionCV(penalty='l1', max_iter=1000, scoring='accuracy', cv=cv, random_state=41, solver='liblinear')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # L1 Regularization naturally performs feature selection by setting some coefficients to zero\n",
    "    selected_features = [feature_names[i] for i in range(X.shape[1]) if model.coef_[0, i] != 0]\n",
    "    train_score = model.score(X,y)\n",
    "    \n",
    "\n",
    "    return model, selected_features, train_score, len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2de008-a57c-49e6-a043-5f031d75a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "best_score = -np.inf\n",
    "best_l, best_num_features, best_features = None, None, None\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for l in l_values:\n",
    "    X, y, feature_names = extract_features(train_dfs, l) \n",
    "    X = scaler.fit_transform(X)  # Normalize features\n",
    "    l1_model, selected_features, avg_score, avg_num_features = l1_logistic_reg(X, y, feature_names)\n",
    "    results.append((l, avg_score, avg_num_features, 1 - avg_score, selected_features))\n",
    "    print(\"Code execution completed for l: \", l)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_l = l\n",
    "        best_num_features = avg_num_features\n",
    "        best_features = selected_features\n",
    "        \n",
    "print(f\"Best l: {best_l}, Best number of selected features (p): {best_num_features}\")\n",
    "print(f\"Best features: {best_features}\")\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}, Best test error: {1 - best_score:.4f}\")\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"l\", \"Train Accuracy\", \"Num Features\", \"Train Error\", \"Selected Features\"])\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e0fe8-98a4-46d0-8edc-24113b3f5b17",
   "metadata": {},
   "source": [
    "We can see that the best cross validation accuracy  is obtained when l = 1. \n",
    "Best l: 1, Best number of selected features (p): 14\n",
    "Train Accuracy: 1.0\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c51634-d61c-4e31-a2ea-8cf49400bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model on best l\n",
    "X_optimal_train, y_optimal_train, all_feature_names = extract_features(train_dfs, best_l)\n",
    "X_optimal_train = scaler.fit_transform(X_optimal_train)  # Normalize train set\n",
    "selected_indices = [all_feature_names.index(f) for f in best_features]  # Get indices of selected features\n",
    "X_optimal_train = X_optimal_train[:, selected_indices]  # Keep only selected features\n",
    "train_model_eval(l1_model, X_optimal_train, y_optimal_train) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8e53e-538e-4409-8d9f-cef4258135df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#First we extract test set features using the best l\n",
    "X_optimal_test, y_optimal_test, all_feature_names = extract_features(test_dfs, best_l)\n",
    "X_optimal_test = scaler.transform(X_optimal_test)  # Normalize test set\n",
    "selected_indices = [all_feature_names.index(f) for f in best_features]  # Get indices of selected features\n",
    "X_optimal_test = X_optimal_test[:, selected_indices]  # Keep only selected features\n",
    "\n",
    "test_model_eval(l1_model, X_optimal_test, y_optimal_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33fbb48-982c-436d-abde-5bf5be5e3248",
   "metadata": {},
   "source": [
    "Test score has reduced to 0.95 for test data with L1 penalized.\n",
    "-\n",
    "\n",
    "Model in 2A: Train and Test model scores: 1.0. \n",
    "Here, 2V) L1 Penalized model: Train accuracy score: 1.0, Test accuracy score: 0.95\n",
    "-\n",
    "\n",
    "Logistic regression without the L1 penalized performs better wrt. test accuracy score. .\n",
    "--\n",
    "\n",
    "Note: CANNOT COMPARE P VALUES. DUE TO SYSTEM THAT I AM RUNNING THIS ON, P-VALUES ARE NOT COMPUTER CORRECTLY (ALL DISPLAYED AS 1.0)\n",
    "\n",
    "Next, we evaluate models with multi-class classification instead of binary. \n",
    "--\n",
    "C) Model 3: Multiclass L1 Penalized\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677f45a-d25d-40ab-bf04-473f9a030cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_multiclass(dataframes, l):\n",
    "    \"\"\"\n",
    "    Extracts time-series features for multi-class classification.\n",
    "    Combines 'bending1' and 'bending2' into a single 'bending' class. \n",
    "    Converts all labels to numeric values\n",
    "    \"\"\"\n",
    "    feature_matrix = []\n",
    "    labels = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Define feature names once using the first dataframe\n",
    "    first_df = dataframes[0]\n",
    "    for col in first_df.columns:\n",
    "        if col not in [\"activity\", \"time\"]:\n",
    "            for i in range(l):\n",
    "                feature_names.extend([\n",
    "                    f\"{col}_min_seg{i}\", f\"{col}_max_seg{i}\", f\"{col}_mean_seg{i}\",\n",
    "                    f\"{col}_median_seg{i}\", f\"{col}_std_seg{i}\", f\"{col}_p25_seg{i}\", f\"{col}_p75_seg{i}\"\n",
    "                ])\n",
    "    \n",
    "    # Process each dataframe\n",
    "    for df in dataframes:\n",
    "        activity = df[\"activity\"].iloc[0]\n",
    "        label = \"bending\" if activity in [\"bending1\", \"bending2\"] else activity\n",
    "        labels.append(label)\n",
    "        \n",
    "        indices = np.linspace(0, len(df), l+1, dtype=int)\n",
    "        features = []\n",
    "        \n",
    "        for i in range(l):\n",
    "            segment = df.iloc[indices[i]: indices[i+1]]\n",
    "            for col in df.columns:\n",
    "                if col not in [\"activity\", \"time\"]:\n",
    "                    features.extend([\n",
    "                        np.min(segment[col]), np.max(segment[col]), np.mean(segment[col]),\n",
    "                        np.median(segment[col]), np.std(segment[col]),\n",
    "                        np.percentile(segment[col], 25), np.percentile(segment[col], 75)\n",
    "                    ])\n",
    "        \n",
    "        feature_matrix.append(features)\n",
    "    \n",
    "    return np.array(feature_matrix), np.array(labels), feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba9ce5-9ccc-453d-ab93-e0f980403c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_multiclass(model, X, y, y_numeric,unique_labels):\n",
    "    \"\"\"\n",
    "    Evaluates a trained multiclass model using Confusion Matrix and ROC curves.\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    # Compute accuracy\n",
    "    score = model.score(X, y_numeric)\n",
    "    print(\"Accuracy: \", score)\n",
    "    \n",
    "    # Compute Confusion Matrix\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    cm = confusion_matrix(y_numeric, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    y_bin = label_binarize(y_numeric, classes=range(len(unique_labels)))\n",
    "    y_score = model.predict_proba(X)\n",
    "    \n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(len(unique_labels)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple'])\n",
    "    for i, color in zip(range(len(unique_labels)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multiclass ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08460ec6-cb0a-4db9-9d0d-1b3d67e3ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_multiclass_logistic_reg(X, y, feature_names, cv_splits=5):\n",
    "    \"\"\"\n",
    "    Trains L1-penalized Logistic Regression using L1 regularization for feature selection.\n",
    "    Returns trained model, selected features, and test accuracy. \n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=41)\n",
    "    model = LogisticRegressionCV(penalty='l1', max_iter=50000, tol=1e-4, scoring='accuracy', cv=cv, random_state=41, solver='saga')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    selected_features = [feature_names[i] for i in range(X.shape[1]) if model.coef_[0, i] != 0]\n",
    "    \n",
    "    train_score = model.score(X, y)\n",
    "    \n",
    "    return model, selected_features, train_score, len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793adaa-2e41-4426-8737-2cd60702cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_score = -np.inf\n",
    "best_l, best_num_features, best_features = None, None, None\n",
    "results = []\n",
    "\n",
    "for l in l_values:\n",
    "    X, y, feature_names = extract_features_multiclass(train_dfs, l) \n",
    "\n",
    "    # Convert class labels to numeric values for multiclass classification\n",
    "    unique_labels = list(set(y))\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y_numeric = np.array([label_mapping[label] for label in y])\n",
    "    \n",
    "    X = scaler.fit_transform(X)\n",
    "    l1_multiclass_model, selected_features, avg_score, avg_num_features = l1_multiclass_logistic_reg(X, y_numeric, feature_names)\n",
    "    results.append((l, avg_score, avg_num_features, 1 - avg_score, selected_features))\n",
    "    print(\"Code execution completed for l: \", l)\n",
    "\n",
    "    #Find l with best score\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_l = l\n",
    "        best_num_features = avg_num_features\n",
    "        best_features = selected_features\n",
    "\n",
    "print(f\"Best l: {best_l}, Best number of selected features (p): {best_num_features}\")\n",
    "print(f\"Best features: {best_features}\")\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}, Best test error: {1 - best_score:.4f}\")\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"l\", \"Train Accuracy\", \"Num Features\", \"Train Error\", \"Selected Features\"])\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13a3e8-6023-4139-b00e-fc0d69535839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on training set\n",
    "\n",
    "#First Extract Features for best l \n",
    "X_train, y_train, feature_train = extract_features_multiclass(train_dfs, best_l)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "unique_labels = list(set(y_train))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train_numeric = np.array([label_mapping[label] for label in y_train])\n",
    "\n",
    "#Fit model on best l\n",
    "l1_multiclass_model.fit(X_train, y_train_numeric)\n",
    "train_score = l1_multiclass_model.score(X_train, y_train_numeric)\n",
    "print (\"Train accuracy:\", train_score)\n",
    "\n",
    "#Evaluate performance on test set \n",
    "\n",
    "#First extract features in test set\n",
    "X_test, y_test, feature_test = extract_features_multiclass(test_dfs, best_l)\n",
    "X_test = scaler.transform(X_test)\n",
    "unique_labels = list(set(y_test))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_test_numeric = np.array([label_mapping[label] for label in y_test])\n",
    "\n",
    "test_score = l1_multiclass_model.score(X_test, y_test_numeric)\n",
    "print (\"Test accuracy: \", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9074523-546e-4656-80c4-66cfb109ab0a",
   "metadata": {},
   "source": [
    "With L1 penalized multiclass, best train accuracy = 1.0, best test accuracy = 0.84. \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae01c0-f351-4e9b-8dc1-f985dd0a39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC curves and calculate Confusion Matrix for Train and Test for L1 penalized multiclass model: \n",
    "print(\"ROC and Confusion Matrix for TRAINING DATA\")\n",
    "evaluate_multiclass(l1_multiclass_model, X_train, y_train, y_train_numeric, unique_labels)\n",
    "print(\"ROC and Confusion Matrix for TEST DATA\")\n",
    "evaluate_multiclass(l1_multiclass_model, X_test, y_test, y_test_numeric, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07a299-b142-4f27-a399-b41024d58bf8",
   "metadata": {},
   "source": [
    "D)  Naive Bayes Model with Multinomial Priors\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7a923-62d9-4a82-bc3e-326054e1e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multinomial_naive_bayes(X, y, cv_splits=5):\n",
    "    \"\"\"\n",
    "    Builds Naive Bayes model using Multinomial priors\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=41)\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X, y)\n",
    "    train_score = model.score(X, y)\n",
    "    return model, train_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebb4df-3381-4032-bdde-a781a94183dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "best_score = -np.inf\n",
    "best_l, best_num_features, best_features = None, None, None\n",
    "\n",
    "for l in l_values:\n",
    "    X, y, feature_names = extract_features_multiclass(train_dfs, l)\n",
    "    # Convert labels to numeric values for multiclass classification\n",
    "    unique_labels = list(set(y))\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y_numeric = np.array([label_mapping[label] for label in y])\n",
    "    NB_model, avg_score = multinomial_naive_bayes(X, y_numeric)\n",
    "    results.append((l, avg_score, len(feature_names), 1 - avg_score))\n",
    "    print(\"Code execution completed for l: \", l)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_l = l\n",
    "        best_num_features = len(feature_names)\n",
    "print(\"Naive Bayes Classifier using Multinomial Distribution\")       \n",
    "print(f\"Best l: {best_l}, Best number of selected features (p): {best_num_features}\")\n",
    "\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}, Best test error: {1 - best_score:.4f}\")\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"l\", \"Train Accuracy\", \"Num Features\", \"Train Error\"])\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585eb7fc-d8a8-49b7-8915-bb5d2a3875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on training set\n",
    "print(\"Performance of Naive Bayes model with Multinomial priors\")\n",
    "#First Extract Features for best l = 6\n",
    "X_train, y_train, feature_train = extract_features_multiclass(train_dfs, best_l)\n",
    "unique_labels = list(set(y_train))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train_numeric = np.array([label_mapping[label] for label in y_train])\n",
    "\n",
    "#Fit model on best l\n",
    "NB_model.fit(X_train, y_train_numeric)\n",
    "train_score = NB_model.score(X_train, y_train_numeric)\n",
    "print (\"Train accuracy:\", train_score)\n",
    "\n",
    "#Evaluate performance on test set \n",
    "\n",
    "#First extract features in test set\n",
    "X_test, y_test, feature_test = extract_features_multiclass(test_dfs, best_l)\n",
    "unique_labels = list(set(y_test))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_test_numeric = np.array([label_mapping[label] for label in y_test])\n",
    "\n",
    "test_score = NB_model.score(X_test, y_test_numeric)\n",
    "print (\"Test accuracy: \", test_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef1abf-48e6-4de6-b7ca-3c951654f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC curves and calculate Confusion Matrix for Train and Test for Naive Bayes Multinomial priors:\n",
    "print(\"ROC and Confusion Matrix for TRAINING DATA for Naive Bayes Multinomial Priors\")\n",
    "evaluate_multiclass(NB_model, X_train, y_train, y_train_numeric, unique_labels)\n",
    "print(\"ROC and Confusion Matrix for TEST DATA Naive Bayes Multinomial Priors\")\n",
    "evaluate_multiclass(NB_model, X_test, y_test,y_test_numeric, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1052fb-f517-490d-9e9c-c2d810d07844",
   "metadata": {},
   "source": [
    "E) Naive Bayes Model with Gaussian Priors: \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51b8bd-5772-4e15-8943-b2e7e54abc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_naive_bayes(X, y, cv_splits=5):\n",
    "    \"\"\"\n",
    "    Builds Naive Bayes model using Gaussian\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=41)\n",
    "    model = GaussianNB()\n",
    "    model.fit(X, y)\n",
    "    train_score = model.score(X, y)\n",
    "    return model, train_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2893e15-89e3-4186-a0fe-f2fa8d891cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "best_score = -np.inf\n",
    "best_l, best_num_features, best_features = None, None, None\n",
    "\n",
    "for l in l_values:\n",
    "    X, y, feature_names = extract_features_multiclass(train_dfs, l)\n",
    "    # Convert labels to numeric values for multiclass classification\n",
    "    unique_labels = list(set(y))\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y_numeric = np.array([label_mapping[label] for label in y])\n",
    "    gaussian_model, avg_score = gaussian_naive_bayes(X, y_numeric)\n",
    "    results.append((l, avg_score, len(feature_names), 1 - avg_score))\n",
    "    print(\"Code execution completed for l: \", l)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_l = l\n",
    "        best_num_features = len(feature_names)\n",
    "print(\"Naive Bayes Classifier using Gaussian Distribution\")       \n",
    "print(f\"Best l: {best_l}, Best number of selected features (p): {best_num_features}\")\n",
    "\n",
    "print(f\"Best cross-validation accuracy: {best_score:.4f}, Best test error: {1 - best_score:.4f}\")\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"l\", \"Train Accuracy\", \"Num Features\", \"Train Error\"])\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef352c-a694-4383-b7b3-514a76dc795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on training set\n",
    "\n",
    "#First Extract Features for best l. \n",
    "X_train, y_train, feature_train = extract_features_multiclass(train_dfs, best_l)\n",
    "unique_labels = list(set(y_train))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_train_numeric = np.array([label_mapping[label] for label in y_train])\n",
    "\n",
    "#Fit model on best l\n",
    "gaussian_model.fit(X_train, y_train_numeric)\n",
    "y_train_pred = gaussian_model.predict(X_train)\n",
    "train_score = gaussian_model.score(X_train, y_train_numeric)\n",
    "print (\"Train accuracy:\", train_score)\n",
    "\n",
    "#Evaluate performance on test set \n",
    "\n",
    "#First extract features in test set\n",
    "X_test, y_test, feature_test = extract_features_multiclass(test_dfs, best_l)\n",
    "unique_labels = list(set(y_test))\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "y_test_numeric = np.array([label_mapping[label] for label in y_test])\n",
    "\n",
    "test_score = gaussian_model.score(X_test, y_test_numeric)\n",
    "print (\"Test accuracy: \", test_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb124d5-771c-4e00-8ba4-7a6f83e8e04a",
   "metadata": {},
   "source": [
    "ROC Curves and Confusion Matrix for Naive Bayes Gaussian Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d530de9-85fd-4f58-8167-92813fe5668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC curves and calculate Confusion Matrix for Train and Test for Naive Bayes Gaussian priors:\n",
    "print(\"ROC and Confusion Matrix for TRAINING DATA\")\n",
    "evaluate_multiclass(gaussian_model, X_train, y_train, y_train_numeric, unique_labels)\n",
    "print(\"ROC and Confusion Matrix for TEST DATA\")\n",
    "evaluate_multiclass(gaussian_model, X_test, y_test, y_test_numeric, unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159682c-72ad-4a7e-b6f6-bc099d3dda3f",
   "metadata": {},
   "source": [
    "Summary of Multiclass models: All multiclass models have the same test error\n",
    "--\n",
    "\n",
    "1. L1 Penalized multiclass Regression: Training Error: 1.0, Test error: 0.84\n",
    "   -\n",
    "3. Naive Bayes (Multinomial Priors): Training error: 0.96, 0.84\n",
    "   --\n",
    "5. Naive Bayes (Gaussian Priors): Training error: 1.0, Test error: 0.84\n",
    "   --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a2dcd-70bb-4b7b-b59d-22a2acfed458",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------\n",
    "Sources\n",
    "\n",
    "ESL 7.10.2\n",
    "\n",
    "https://www.kaggle.com/code/magedmallek/feature-selection-explained-rfecv\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "\n",
    "https://medium.com/@arnavr/scikit-learn-solvers-explained-780a17bc322d\n",
    "\n",
    "https://www.geeksforgeeks.org/python-list-extend-method/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html\n",
    "\n",
    "https://www.geeksforgeeks.org/recursive-feature-elimination-with-cross-validation-in-scikit-learn/\n",
    "\n",
    "https://www.geeksforgeeks.org/comparing-various-online-solvers-in-scikit-learn/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html\n",
    "\n",
    "https://www.kaggle.com/code/nadare/simple-logistic-regression-with-l1-penalty/notebook?scriptVersionId=10368578\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
